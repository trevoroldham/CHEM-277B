{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb6f6a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trevor/opt/miniconda3/envs/msse-python/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pyanitools as pya\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "# Set the HDF5 file containing the data\n",
    "hdf5file = 'ANI-1_release/ani_gdb_s01.h5'\n",
    "\n",
    "# Construct the data loader class\n",
    "adl = pya.anidataloader(hdf5file)\n",
    "\n",
    "p = []\n",
    "x = []\n",
    "e = []\n",
    "s = []\n",
    "sm = []\n",
    "i = 0\n",
    "\n",
    "for data in adl:\n",
    "    if (i >= 10):\n",
    "        break\n",
    "    # Extract the data\n",
    "    p.append(data['path'])\n",
    "    x.append(data['coordinates'])\n",
    "    e.append(data['energies'])\n",
    "    s.append(data['species'])\n",
    "    sm.append(data['smiles'])\n",
    "    i+=1\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Closes the H5 data file\n",
    "adl.cleanup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8a8e80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12e7bc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyanitools import anidataloader\n",
    "# data = anidataloader(\"../../ANI1_dataset/ANI-1_release/ani_gdb_s07.h5\")\n",
    "data = anidataloader(\"ANI-1_release/ani_gdb_s01.h5\")\n",
    "data_iter = data.__iter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69718d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path:    /gdb11_s01/gdb11_s01-0\n",
      "  Smiles:       [H]C([H])([H])[H]\n",
      "  Symbols:      ['C', 'H', 'H', 'H', 'H']\n",
      "  Coordinates:  (5400, 5, 3)\n",
      "  Energies:     (5400,) \n",
      "\n",
      "  Energies:  [-40.48058817 -40.48311923 -40.48473545 ... -40.4961279  -40.45599721\n",
      " -40.46479283]\n"
     ]
    }
   ],
   "source": [
    "mols = next(data_iter)\n",
    "# Extract the data\n",
    "P = mols['path']\n",
    "X = mols['coordinates']\n",
    "E = mols['energies']\n",
    "S = mols['species']\n",
    "sm = mols['smiles']\n",
    "\n",
    "# Print the data\n",
    "print(\"Path:   \", P)\n",
    "print(\"  Smiles:      \",\"\".join(sm))\n",
    "print(\"  Symbols:     \", S)\n",
    "print(\"  Coordinates: \", X.shape)\n",
    "print(\"  Energies:    \", E.shape, \"\\n\")\n",
    "print(\"  Energies: \", E)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab1f6421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "10800\n"
     ]
    }
   ],
   "source": [
    "data_iter = data.__iter__()\n",
    "count = 0\n",
    "count_conf = 0\n",
    "for mol in data_iter:\n",
    "    count += 1\n",
    "    count_conf += len(mol['energies'])\n",
    "print(count)\n",
    "print(count_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a9bf300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def calc_f_C(Rij, RC):\n",
    "    f_C_value = 0.5 * np.cos(np.pi * Rij / RC) + 0.5\n",
    "    indicator = ((Rij <= RC) & (Rij != 0)).astype(float) # Make f_C(0)=0 to make sure the sum in distance conversion function and radial conversion function can run with j=i\n",
    "    return f_C_value * indicator\n",
    "\n",
    "def radial_component(Rijs, eta, Rs, RC=5.2):\n",
    "    # Rijs is a 1d array, all other parameters are scalars\n",
    "    f_C_values = calc_f_C(Rijs, RC)\n",
    "    individual_components = np.exp(-eta * (Rijs - Rs) ** 2) * f_C_values\n",
    "    return np.sum(individual_components)\n",
    "\n",
    "def angular_component(Rij_vectors, Rik_vectors, zeta, theta_s, eta, Rs, RC=3.5):\n",
    "    # Rij_vectors and Rik_vectors are 2d arrays with shape (n_atoms, 3), all other parameters are scalars\n",
    "    # calculate theta_ijk values from vector operations\n",
    "    dot_products = Rij_vectors.dot(Rik_vectors.T)\n",
    "    Rij_norms = np.linalg.norm(Rij_vectors, axis=-1)\n",
    "    Rik_norms = np.linalg.norm(Rik_vectors, axis=-1)\n",
    "    norms = Rij_norms.reshape((-1, 1)).dot(Rik_norms.reshape((1, -1)))\n",
    "    cos_values = np.clip(dot_products / (norms + 1e-8), -1, 1)\n",
    "    theta_ijks = np.arccos(cos_values)\n",
    "    theta_ijk_filter = (theta_ijks != 0).astype(float)\n",
    "    mean_dists = (Rij_norms.reshape((-1, 1)) + Rik_norms.reshape((1, -1))) / 2\n",
    "    f_C_values_Rij = calc_f_C(Rij_norms, RC)\n",
    "    f_C_values_Rik = calc_f_C(Rik_norms, RC)\n",
    "    f_C_values = f_C_values_Rij.reshape((-1, 1)).dot(f_C_values_Rik.reshape((1, -1)))\n",
    "    individual_components = (1 + np.cos(theta_ijks - theta_s)) ** zeta * np.exp(-eta * (mean_dists - Rs) ** 2) * f_C_values * theta_ijk_filter\n",
    "    return 2 ** (1 - zeta) * np.sum(individual_components)\n",
    "\n",
    "def calc_aev(atom_types, coords, i_index):\n",
    "    # atom_types are np.array of ints\n",
    "    relative_coordinates = coords - coords[i_index]\n",
    "    nearby_atom_indicator = np.linalg.norm(relative_coordinates, axis=-1) < 5.3\n",
    "    relative_coordinates = relative_coordinates[nearby_atom_indicator]\n",
    "    atom_types = atom_types[nearby_atom_indicator]\n",
    "    radial_aev = np.array([radial_component(np.linalg.norm(relative_coordinates[atom_types == atom], axis=-1), eta, Rs) \\\n",
    "                           for atom in [0, 1, 2, 3] for eta in [16] \\\n",
    "                           for Rs in [0.900000,1.168750,1.437500,1.706250,1.975000,2.243750,2.51250,2.781250,3.050000,\\\n",
    "                                   3.318750,3.587500,3.856250,4.125000,4.39375,4.662500,4.931250]])\n",
    "    angular_aev = np.array([angular_component(relative_coordinates[atom_types == atom_j], relative_coordinates[atom_types == atom_k],\\\n",
    "                                             zeta, theta_s, eta, Rs) \\\n",
    "                            for atom_j in [0, 1, 2, 3] for atom_k in range(atom_j, 4) for zeta in [32] \\\n",
    "                            for theta_s in [0.19634954,0.58904862,0.9817477,1.3744468,1.7671459,2.1598449,2.552544,2.945243]\\\n",
    "                            for eta in [8] for Rs in [0.900000,1.550000,2.200000,2.850000]])\n",
    "#     print(len(radial_aev), len(angular_aev))\n",
    "    return np.concatenate([radial_aev, angular_aev])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cbb303c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5400, 5, 3)\n",
      "(384,)\n"
     ]
    }
   ],
   "source": [
    "mapping={\"H\":0, \"C\":1, \"N\":2, \"O\":3}\n",
    "print(X.shape)\n",
    "elements= np.array([mapping[atom] for atom in S])\n",
    "X_train = calc_aev(elements, X[0], 2)\n",
    "print(X_train.shape)\n",
    "#print(X_train)\n",
    "#y_train = E[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17bcbeea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7124dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a6a64c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4a7005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchani \n",
    "import torch\n",
    "import numpy as np\n",
    "from pyanitools import anidataloader\n",
    "# data = anidataloader(\"../../ANI1_dataset/ANI-1_release/ani_gdb_s07.h5\")\n",
    "data = anidataloader(\"ani_gdb_s05.h5\")\n",
    "data_iter = data.__iter__()\n",
    "mols = next(data_iter)\n",
    "# Extract the data\n",
    "P = mols['path']\n",
    "X = mols['coordinates']\n",
    "E = mols['energies']\n",
    "S = mols['species']\n",
    "sm = mols['smiles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04726d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "aev_converter = torchani.AEVComputer(Rcr, Rca, EtaR, ShfR, EtaA, Zeta, ShfA, ShfZ, num_species)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd09ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = 0\n",
    "data_iter = data.__iter__()\n",
    "\n",
    "atom_energies = []\n",
    "atom_symbols = []\n",
    "atom_aevs = [] \n",
    "while item < 2:\n",
    "    mols = next(data_iter)\n",
    "    # Extract the data\n",
    "    P = mols['path']\n",
    "    X = mols['coordinates']\n",
    "    E = mols['energies']\n",
    "    S = mols['species']\n",
    "    print('Path = ', P)\n",
    "    print('Species = ', S)\n",
    "    item += 1\n",
    "    coords = torch.tensor(X)\n",
    "    elements= np.array([mapping[atom] for atom in S])\n",
    "    print(elements)\n",
    "    species, aevs = aev_converter.forward((S, X))\n",
    "    atom_aevs.append(aevs)\n",
    "    atom_symbols.append(species)\n",
    "    atom_energies.append(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "191960d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4538b3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANI(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sub_nets = nn.ModuleDict({\n",
    "            \"C\": ANI_sub(),\n",
    "            \"H\": ANI_sub(),\n",
    "            \"N\": ANI_sub(),\n",
    "            \"O\": ANI_sub()})\n",
    "\n",
    "        print(self.sub_nets)\n",
    "    def forward(self, aevs, atom_types):\n",
    "        atomic_energies = [self.sub_nets[atom_types][i][aevs[i]] for i in range(len(aevs))]\n",
    "        \n",
    "        total_energies = torch.sum(atomic_energies,dim=...)\n",
    "        return total_energies\n",
    "\n",
    "class ANI_sub(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(384, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, aev):\n",
    "        atomic_energy = self.layers[aev]\n",
    "        return atomic_energy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "12234374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleDict(\n",
      "  (C): ANI_sub(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (3): GELU(approximate='none')\n",
      "      (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (5): GELU(approximate='none')\n",
      "      (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (H): ANI_sub(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (3): GELU(approximate='none')\n",
      "      (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (5): GELU(approximate='none')\n",
      "      (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (N): ANI_sub(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (3): GELU(approximate='none')\n",
      "      (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (5): GELU(approximate='none')\n",
      "      (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (O): ANI_sub(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (3): GELU(approximate='none')\n",
      "      (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (5): GELU(approximate='none')\n",
      "      (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [147]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchsummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m ANI()\n\u001b[0;32m----> 3\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m384\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melements\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/msse-python/lib/python3.9/site-packages/torchsummary/torchsummary.py:136\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_data, batch_dim, branching, col_names, col_width, depth, device, dtypes, verbose, *args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 136\u001b[0m x, input_size \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_input_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m set_device(args, device), set_device(kwargs, device)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/msse-python/lib/python3.9/site-packages/torchsummary/torchsummary.py:215\u001b[0m, in \u001b[0;36mprocess_input_data\u001b[0;34m(input_data, batch_dim, device, dtypes)\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m             dtypes \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mfloat] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_data)\n\u001b[0;32m--> 215\u001b[0m         input_size \u001b[38;5;241m=\u001b[39m \u001b[43mget_correct_input_sizes\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m         x \u001b[38;5;241m=\u001b[39m get_input_tensor(input_size, batch_dim, dtypes, device)\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/msse-python/lib/python3.9/site-packages/torchsummary/torchsummary.py:261\u001b[0m, in \u001b[0;36mget_correct_input_sizes\u001b[0;34m(input_size)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    259\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m item\n\u001b[0;32m--> 261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m input_size \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43many\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput_data is invalid, or negative size found in input_data.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(input_size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(input_size[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mint\u001b[39m):\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "model = ANI()\n",
    "summary(model, [(1, 384,), elements])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "cdc3b14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "from time import time\n",
    "\n",
    "def timing(f):\n",
    "    @wraps(f)\n",
    "    def wrap(*args, **kw):\n",
    "        ts = time()\n",
    "        result = f(*args, **kw)\n",
    "        te = time()\n",
    "        print('func:%r  took: %2.4f sec' % (f.__name__,  te-ts))\n",
    "        return result\n",
    "    return wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "349e950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD, Adam\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "def create_chunks(complete_list, chunk_size=None, num_chunks=None):\n",
    "    '''\n",
    "    Cut a list into multiple chunks, each having chunk_size (the last chunk might be less than chunk_size) or having a total of num_chunk chunks\n",
    "    '''\n",
    "    chunks = []\n",
    "    if num_chunks is None:\n",
    "        num_chunks = math.ceil(len(complete_list) / chunk_size)\n",
    "    elif chunk_size is None:\n",
    "        chunk_size = math.ceil(len(complete_list) / num_chunks)\n",
    "    for i in range(num_chunks):\n",
    "        chunks.append(complete_list[i * chunk_size: (i + 1) * chunk_size])\n",
    "    return chunks\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, model, optimizer_type, learning_rate, epoch, batch_size, input_transform=lambda x: x,):\n",
    "        \"\"\" The class for training the model\n",
    "        model: nn.Module\n",
    "            A pytorch model\n",
    "        optimizer_type: 'adam' or 'sgd'\n",
    "        learning_rate: float\n",
    "        epoch: int\n",
    "        batch_size: int\n",
    "        input_transform: func\n",
    "            transforming input. Can do reshape here\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        if optimizer_type == \"sgd\":\n",
    "            self.optimizer = SGD(model.parameters(), learning_rate,momentum=0.9)\n",
    "        elif optimizer_type == \"adam\":\n",
    "            self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            \n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.input_transform = input_transform\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "\n",
    "    @timing\n",
    "    def train(self, inputs, outputs, val_inputs, val_outputs,early_stop=False,l2=False,silent=False):\n",
    "        \"\"\" train self.model with specified arguments\n",
    "        inputs: np.array, The shape of input_transform(input) should be (ndata,nfeatures)\n",
    "        outputs: np.array shape (ndata,)\n",
    "        val_nputs: np.array, The shape of input_transform(val_input) should be (ndata,nfeatures)\n",
    "        val_outputs: np.array shape (ndata,)\n",
    "        early_stop: bool\n",
    "        l2: bool\n",
    "        silent: bool. Controls whether or not to print the train and val error during training\n",
    "        \n",
    "        @return\n",
    "        a dictionary of arrays with train and val losses and accuracies\n",
    "        \"\"\"\n",
    "        ### convert data to tensor of correct shape and type here ###\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "        outputs = torch.tensor(outputs, dtype=torch.long)\n",
    "        val_inputs = torch.tensor(val_inputs, dtype=torch.float32)\n",
    "        val_outputs = torch.tensor(val_outputs, dtype=torch.long)\n",
    "        \n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        val_losses = []\n",
    "        val_accuracies = []\n",
    "        weights = self.model.state_dict()\n",
    "        lowest_val_loss = np.inf\n",
    "        \n",
    "        for n_epoch in tqdm(range(self.epoch), leave=False):\n",
    "            self.model.train()\n",
    "            batch_indices = list(range(inputs.shape[0]))\n",
    "            random.shuffle(batch_indices)\n",
    "            batch_indices = create_chunks(batch_indices, chunk_size=self.batch_size)\n",
    "            epoch_loss = 0\n",
    "            epoch_acc = 0\n",
    "            for batch in batch_indices:\n",
    "                batch_importance = len(batch) / len(outputs)\n",
    "                batch_input = inputs[batch]\n",
    "                batch_output = outputs[batch]\n",
    "                ### make prediction and compute loss with loss function of your choice on this batch ###\n",
    "                batch_predictions = self.model.forward(batch_input)\n",
    "                loss_func = nn.CrossEntropyLoss()\n",
    "                loss = loss_func(batch_predictions, batch_output)\n",
    "                if l2:\n",
    "                    ### Compute the loss with L2 regularization ###\n",
    "                    self.optimizer = torch.optim.Adam(model.parameters(), lr = self.learning_rate, weight_decay = 1e-5)\n",
    "                    loss = loss_func(batch_predictions, batch_output)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                ### Compute epoch_loss and epoch_acc\n",
    "            epoch_loss, epoch_acc = self.evaluate(inputs, outputs)  \n",
    "            val_loss, val_acc = self.evaluate(val_inputs, val_outputs, print_acc=False)\n",
    "            if n_epoch % 10 ==0 and not silent: \n",
    "                print(\"Epoch %d/%d - Loss: %.3f - Acc: %.3f\" % (n_epoch + 1, self.epoch, epoch_loss, epoch_acc))\n",
    "                print(\"              Val_loss: %.3f - Val_acc: %.3f\" % (val_loss, val_acc))\n",
    "            losses.append(epoch_loss.detach().numpy())\n",
    "            accuracies.append(epoch_acc)\n",
    "            val_losses.append(val_loss.detach().numpy())\n",
    "            val_accuracies.append(val_acc)\n",
    "            if early_stop:\n",
    "                if val_loss < lowest_val_loss:\n",
    "                    lowest_val_loss = val_loss\n",
    "                    weights = self.model.state_dict()\n",
    "\n",
    "        if early_stop:\n",
    "            self.model.load_state_dict(weights)    \n",
    "\n",
    "        return {\"losses\": losses, \"accuracies\": accuracies, \"val_losses\": val_losses, \"val_accuracies\": val_accuracies}\n",
    "        \n",
    "    def evaluate(self, inputs, outputs, print_acc=False):\n",
    "        \"\"\" evaluate model on provided input and output\n",
    "        inputs: np.array, The shape of input_transform(input) should be (ndata,nfeatures)\n",
    "        outputs: np.array shape (ndata,)\n",
    "        print_acc: bool\n",
    "        \n",
    "        @return\n",
    "        losses: float\n",
    "        acc: float\n",
    "        \"\"\"\n",
    "\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "        outputs = torch.tensor(outputs, dtype=torch.long)\n",
    "\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "        \n",
    "        pred = self.model.forward(inputs)\n",
    "\n",
    "        losses = loss_func(pred, outputs)\n",
    "        #print(\"pred = \", pred)\n",
    "        #print(\"truth = \" ,outputs)\n",
    "        \n",
    "        sum = 0\n",
    "        for i in range(len(outputs)):\n",
    "            if outputs[i] == torch.argmax(pred[i]):\n",
    "                sum += 1\n",
    "        acc = sum / len(outputs)\n",
    "        if print_acc:\n",
    "            print(\"Accuracy: %.3f\" % acc)\n",
    "        return losses, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6352d369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de632031",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
