{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Importing ANI Tools and Viewing Data"
      ],
      "metadata": {
        "id": "dfjPzdgl1WA3"
      },
      "id": "dfjPzdgl1WA3"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bb6f6a4e",
      "metadata": {
        "id": "bb6f6a4e"
      },
      "outputs": [],
      "source": [
        "import pyanitools as pya\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sys import exit\n",
        "\n",
        "# Set the HDF5 file containing the data\n",
        "hdf5file = 'ani_gdb_s01.h5'\n",
        "\n",
        "# Construct the data loader class\n",
        "adl = pya.anidataloader(hdf5file)\n",
        "\n",
        "p = []\n",
        "x = []\n",
        "e = []\n",
        "s = []\n",
        "sm = []\n",
        "i = 0\n",
        "\n",
        "for data in adl:\n",
        "    if (i >= 10):\n",
        "        break\n",
        "    # Extract the data\n",
        "    p.append(data['path'])\n",
        "    x.append(data['coordinates'])\n",
        "    e.append(data['energies'])\n",
        "    s.append(data['species'])\n",
        "    sm.append(data['smiles'])\n",
        "    i+=1\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "# Closes the H5 data file\n",
        "adl.cleanup()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1fc76e4a",
      "metadata": {
        "id": "1fc76e4a"
      },
      "outputs": [],
      "source": [
        "from pyanitools import anidataloader\n",
        "# data = anidataloader(\"../../ANI1_dataset/ANI-1_release/ani_gdb_s07.h5\")\n",
        "data = anidataloader(\"ani_gdb_s01.h5\")\n",
        "data_iter = data.__iter__()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "afbfde5e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afbfde5e",
        "outputId": "83b84887-d9e5-48af-dba5-1a4b911873e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path:    /gdb11_s01/gdb11_s01-0\n",
            "  Smiles:       [H]C([H])([H])[H]\n",
            "  Symbols:      ['C', 'H', 'H', 'H', 'H']\n",
            "  Coordinates:  (5400, 5, 3)\n",
            "  Energies:     (5400,) \n",
            "\n",
            "  Energies:  [-40.48058817 -40.48311923 -40.48473545 ... -40.4961279  -40.45599721\n",
            " -40.46479283]\n"
          ]
        }
      ],
      "source": [
        "mols = next(data_iter)\n",
        "# Extract the data\n",
        "P = mols['path']\n",
        "X = mols['coordinates']\n",
        "E = mols['energies']\n",
        "S = mols['species']\n",
        "sm = mols['smiles']\n",
        "\n",
        "# Print the data\n",
        "print(\"Path:   \", P)\n",
        "print(\"  Smiles:      \",\"\".join(sm))\n",
        "print(\"  Symbols:     \", S)\n",
        "print(\"  Coordinates: \", X.shape)\n",
        "print(\"  Energies:    \", E.shape, \"\\n\")\n",
        "print(\"  Energies: \", E)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ab1f6421",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab1f6421",
        "outputId": "d1a3c1fe-3697-48eb-ec4d-76c1e784ba96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "10800\n"
          ]
        }
      ],
      "source": [
        "data_iter = data.__iter__()\n",
        "count = 0\n",
        "count_conf = 0\n",
        "for mol in data_iter:\n",
        "    count += 1\n",
        "    count_conf += len(mol['energies'])\n",
        "print(count)\n",
        "print(count_conf)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining AEV Calculation"
      ],
      "metadata": {
        "id": "rL8Xv2PA1eG0"
      },
      "id": "rL8Xv2PA1eG0"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3a9bf300",
      "metadata": {
        "id": "3a9bf300"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def calc_f_C(Rij, RC):\n",
        "    f_C_value = 0.5 * np.cos(np.pi * Rij / RC) + 0.5\n",
        "    indicator = ((Rij <= RC) & (Rij != 0)).astype(float) # Make f_C(0)=0 to make sure the sum in distance conversion function and radial conversion function can run with j=i\n",
        "    return f_C_value * indicator\n",
        "\n",
        "def radial_component(Rijs, eta, Rs, RC=5.2):\n",
        "    # Rijs is a 1d array, all other parameters are scalars\n",
        "    f_C_values = calc_f_C(Rijs, RC)\n",
        "    individual_components = np.exp(-eta * (Rijs - Rs) ** 2) * f_C_values\n",
        "    return np.sum(individual_components)\n",
        "\n",
        "def angular_component(Rij_vectors, Rik_vectors, zeta, theta_s, eta, Rs, RC=3.5):\n",
        "    # Rij_vectors and Rik_vectors are 2d arrays with shape (n_atoms, 3), all other parameters are scalars\n",
        "    # calculate theta_ijk values from vector operations\n",
        "    dot_products = Rij_vectors.dot(Rik_vectors.T)\n",
        "    Rij_norms = np.linalg.norm(Rij_vectors, axis=-1)\n",
        "    Rik_norms = np.linalg.norm(Rik_vectors, axis=-1)\n",
        "    norms = Rij_norms.reshape((-1, 1)).dot(Rik_norms.reshape((1, -1)))\n",
        "    cos_values = np.clip(dot_products / (norms + 1e-8), -1, 1)\n",
        "    theta_ijks = np.arccos(cos_values)\n",
        "    theta_ijk_filter = (theta_ijks != 0).astype(float)\n",
        "    mean_dists = (Rij_norms.reshape((-1, 1)) + Rik_norms.reshape((1, -1))) / 2\n",
        "    f_C_values_Rij = calc_f_C(Rij_norms, RC)\n",
        "    f_C_values_Rik = calc_f_C(Rik_norms, RC)\n",
        "    f_C_values = f_C_values_Rij.reshape((-1, 1)).dot(f_C_values_Rik.reshape((1, -1)))\n",
        "    individual_components = (1 + np.cos(theta_ijks - theta_s)) ** zeta * np.exp(-eta * (mean_dists - Rs) ** 2) * f_C_values * theta_ijk_filter\n",
        "    return 2 ** (1 - zeta) * np.sum(individual_components)\n",
        "\n",
        "def calc_aev(atom_types, coords, i_index):\n",
        "    # atom_types are np.array of ints\n",
        "    relative_coordinates = coords - coords[i_index]\n",
        "    nearby_atom_indicator = np.linalg.norm(relative_coordinates, axis=-1) < 5.3\n",
        "    relative_coordinates = relative_coordinates[nearby_atom_indicator]\n",
        "    atom_types = atom_types[nearby_atom_indicator]\n",
        "    radial_aev = np.array([radial_component(np.linalg.norm(relative_coordinates[atom_types == atom], axis=-1), eta, Rs) \\\n",
        "                           for atom in [0, 1, 2, 3] for eta in [16] \\\n",
        "                           for Rs in [0.900000,1.168750,1.437500,1.706250,1.975000,2.243750,2.51250,2.781250,3.050000,\\\n",
        "                                   3.318750,3.587500,3.856250,4.125000,4.39375,4.662500,4.931250]])\n",
        "    angular_aev = np.array([angular_component(relative_coordinates[atom_types == atom_j], relative_coordinates[atom_types == atom_k],\\\n",
        "                                             zeta, theta_s, eta, Rs) \\\n",
        "                            for atom_j in [0, 1, 2, 3] for atom_k in range(atom_j, 4) for zeta in [32] \\\n",
        "                            for theta_s in [0.19634954,0.58904862,0.9817477,1.3744468,1.7671459,2.1598449,2.552544,2.945243]\\\n",
        "                            for eta in [8] for Rs in [0.900000,1.550000,2.200000,2.850000]])\n",
        "#     print(len(radial_aev), len(angular_aev))\n",
        "    return np.concatenate([radial_aev, angular_aev])\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Torchani and AEVComputer"
      ],
      "metadata": {
        "id": "dWYKTgrU1ioT"
      },
      "id": "dWYKTgrU1ioT"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchani"
      ],
      "metadata": {
        "id": "0sTXGj3DzxxD"
      },
      "id": "0sTXGj3DzxxD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "7c4a7005",
      "metadata": {
        "id": "7c4a7005"
      },
      "outputs": [],
      "source": [
        "import torchani\n",
        "from torchani import AEVComputer\n",
        "Rcr = 5.2\n",
        "EtaR = torch.tensor([16], dtype=torch.float)\n",
        "ShfR = torch.tensor([0.900000,1.168750,1.437500,1.706250,1.975000,2.243750,2.51250,2.781250,3.050000,3.318750,3.587500,3.856250,4.125000,4.39375,4.662500,4.931250])\n",
        "Rca = 3.5\n",
        "EtaA = torch.tensor([8], dtype=torch.float)\n",
        "ShfA = torch.tensor([0.900000,1.550000,2.200000,2.850000], dtype=torch.float)\n",
        "ShfZ = torch.tensor([0.19634954,0.58904862,0.9817477,1.3744468,1.7671459,2.1598449,2.552544,2.945243]) \n",
        "Zeta = torch.tensor([32], dtype=torch.float)\n",
        "species_order = ['H', 'C', 'N', 'O']\n",
        "num_species = len(species_order)\n",
        "\n",
        "aev_computer = torchani.AEVComputer(Rcr, Rca, EtaR, ShfR, EtaA, ShfA, ShfZ, Zeta, num_species)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "93bdffd6",
      "metadata": {
        "id": "93bdffd6"
      },
      "outputs": [],
      "source": [
        "aev_converter = torchani.AEVComputer(Rcr, Rca, EtaR, ShfR, EtaA, Zeta, ShfA, ShfZ, num_species)\n",
        "mapping = {\"H\": 0, \"C\" : 1, \"N\": 2, \"O\": 3}\n",
        "species = np.array([mapping[atom] for atom in S])\n",
        "species = np.tile(species, (X.shape[0], 1))\n",
        "species = torch.tensor(species)\n",
        "X = torch.tensor(X)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aev_output = aev_computer((species, X)) #SPecies: (Number, Atoms) A in [0, 1, 2, 3] Coords: (N, A, 3) Output : (N, A, 384)\n",
        "aev_output[1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19NdXE4zz-Af",
        "outputId": "6427860d-deb8-462d-cf72-602d65d8ee7f"
      },
      "id": "19NdXE4zz-Af",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5400, 5, 384])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a349c73a",
      "metadata": {
        "id": "a349c73a"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining MLP Model with Torch"
      ],
      "metadata": {
        "id": "cx-V-jD11qFu"
      },
      "id": "cx-V-jD11qFu"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "4538b3ea",
      "metadata": {
        "id": "4538b3ea"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class ANI(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.sub_nets = nn.ModuleDict({\n",
        "            \"C\": ANI_sub(),\n",
        "            \"H\": ANI_sub(),\n",
        "            \"N\": ANI_sub(),\n",
        "            \"O\": ANI_sub()})\n",
        "\n",
        "        print(self.sub_nets)\n",
        "    def forward(self, aevs, atom_types):\n",
        "        atomic_energies = [self.sub_nets[atom_types][i][aevs[i]] for i in range(len(aevs))]\n",
        "        \n",
        "        total_energies = torch.sum(atomic_energies,dim=...)\n",
        "        return total_energies\n",
        "\n",
        "class ANI_sub(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(384, 128),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, aev):\n",
        "        atomic_energy = self.layers[aev]\n",
        "        return atomic_energy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "ea76fa86",
      "metadata": {
        "id": "ea76fa86",
        "outputId": "76b498ac-c2c3-4a01-c153-cadcc406fa6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ModuleDict(\n",
            "  (C): ANI_sub(\n",
            "    (layers): Sequential(\n",
            "      (0): Linear(in_features=384, out_features=128, bias=True)\n",
            "      (1): GELU(approximate='none')\n",
            "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
            "      (3): GELU(approximate='none')\n",
            "      (4): Linear(in_features=128, out_features=64, bias=True)\n",
            "      (5): GELU(approximate='none')\n",
            "      (6): Linear(in_features=64, out_features=1, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (H): ANI_sub(\n",
            "    (layers): Sequential(\n",
            "      (0): Linear(in_features=384, out_features=128, bias=True)\n",
            "      (1): GELU(approximate='none')\n",
            "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
            "      (3): GELU(approximate='none')\n",
            "      (4): Linear(in_features=128, out_features=64, bias=True)\n",
            "      (5): GELU(approximate='none')\n",
            "      (6): Linear(in_features=64, out_features=1, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (N): ANI_sub(\n",
            "    (layers): Sequential(\n",
            "      (0): Linear(in_features=384, out_features=128, bias=True)\n",
            "      (1): GELU(approximate='none')\n",
            "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
            "      (3): GELU(approximate='none')\n",
            "      (4): Linear(in_features=128, out_features=64, bias=True)\n",
            "      (5): GELU(approximate='none')\n",
            "      (6): Linear(in_features=64, out_features=1, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (O): ANI_sub(\n",
            "    (layers): Sequential(\n",
            "      (0): Linear(in_features=384, out_features=128, bias=True)\n",
            "      (1): GELU(approximate='none')\n",
            "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
            "      (3): GELU(approximate='none')\n",
            "      (4): Linear(in_features=128, out_features=64, bias=True)\n",
            "      (5): GELU(approximate='none')\n",
            "      (6): Linear(in_features=64, out_features=1, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "ANI(\n",
            "  (sub_nets): ModuleDict(\n",
            "    (C): ANI_sub(\n",
            "      (layers): Sequential(\n",
            "        (0): Linear(in_features=384, out_features=128, bias=True)\n",
            "        (1): GELU(approximate='none')\n",
            "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
            "        (3): GELU(approximate='none')\n",
            "        (4): Linear(in_features=128, out_features=64, bias=True)\n",
            "        (5): GELU(approximate='none')\n",
            "        (6): Linear(in_features=64, out_features=1, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (H): ANI_sub(\n",
            "      (layers): Sequential(\n",
            "        (0): Linear(in_features=384, out_features=128, bias=True)\n",
            "        (1): GELU(approximate='none')\n",
            "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
            "        (3): GELU(approximate='none')\n",
            "        (4): Linear(in_features=128, out_features=64, bias=True)\n",
            "        (5): GELU(approximate='none')\n",
            "        (6): Linear(in_features=64, out_features=1, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (N): ANI_sub(\n",
            "      (layers): Sequential(\n",
            "        (0): Linear(in_features=384, out_features=128, bias=True)\n",
            "        (1): GELU(approximate='none')\n",
            "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
            "        (3): GELU(approximate='none')\n",
            "        (4): Linear(in_features=128, out_features=64, bias=True)\n",
            "        (5): GELU(approximate='none')\n",
            "        (6): Linear(in_features=64, out_features=1, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (O): ANI_sub(\n",
            "      (layers): Sequential(\n",
            "        (0): Linear(in_features=384, out_features=128, bias=True)\n",
            "        (1): GELU(approximate='none')\n",
            "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
            "        (3): GELU(approximate='none')\n",
            "        (4): Linear(in_features=128, out_features=64, bias=True)\n",
            "        (5): GELU(approximate='none')\n",
            "        (6): Linear(in_features=64, out_features=1, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "from torchsummary import summary\n",
        "model = ANI()\n",
        "print(model)\n",
        "#summary(model, [(1, 384,), elements])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining Torch Trainer Class"
      ],
      "metadata": {
        "id": "hyahp7X81vU_"
      },
      "id": "hyahp7X81vU_"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "f8fcbb17",
      "metadata": {
        "id": "f8fcbb17"
      },
      "outputs": [],
      "source": [
        "from functools import wraps\n",
        "from time import time\n",
        "\n",
        "def timing(f):\n",
        "    @wraps(f)\n",
        "    def wrap(*args, **kw):\n",
        "        ts = time()\n",
        "        result = f(*args, **kw)\n",
        "        te = time()\n",
        "        print('func:%r  took: %2.4f sec' % (f.__name__,  te-ts))\n",
        "        return result\n",
        "    return wrap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "8153c2ec",
      "metadata": {
        "id": "8153c2ec"
      },
      "outputs": [],
      "source": [
        "from torch.optim import SGD, Adam\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "\n",
        "def create_chunks(complete_list, chunk_size=None, num_chunks=None):\n",
        "    '''\n",
        "    Cut a list into multiple chunks, each having chunk_size (the last chunk might be less than chunk_size) or having a total of num_chunk chunks\n",
        "    '''\n",
        "    chunks = []\n",
        "    if num_chunks is None:\n",
        "        num_chunks = math.ceil(len(complete_list) / chunk_size)\n",
        "    elif chunk_size is None:\n",
        "        chunk_size = math.ceil(len(complete_list) / num_chunks)\n",
        "    for i in range(num_chunks):\n",
        "        chunks.append(complete_list[i * chunk_size: (i + 1) * chunk_size])\n",
        "    return chunks\n",
        "\n",
        "class Trainer():\n",
        "    def __init__(self, model, optimizer_type, learning_rate, epoch, batch_size, input_transform=lambda x: x,):\n",
        "        \"\"\" The class for training the model\n",
        "        model: nn.Module\n",
        "            A pytorch model\n",
        "        optimizer_type: 'adam' or 'sgd'\n",
        "        learning_rate: float\n",
        "        epoch: int\n",
        "        batch_size: int\n",
        "        input_transform: func\n",
        "            transforming input. Can do reshape here\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        if optimizer_type == \"sgd\":\n",
        "            self.optimizer = SGD(model.parameters(), learning_rate,momentum=0.9)\n",
        "        elif optimizer_type == \"adam\":\n",
        "            self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "            \n",
        "        self.epoch = epoch\n",
        "        self.batch_size = batch_size\n",
        "        self.input_transform = input_transform\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "\n",
        "    @timing\n",
        "    def train(self, inputs, outputs, val_inputs, val_outputs,early_stop=False,l2=False,silent=False):\n",
        "        \"\"\" train self.model with specified arguments\n",
        "        inputs: np.array, The shape of input_transform(input) should be (ndata,nfeatures)\n",
        "        outputs: np.array shape (ndata,)\n",
        "        val_nputs: np.array, The shape of input_transform(val_input) should be (ndata,nfeatures)\n",
        "        val_outputs: np.array shape (ndata,)\n",
        "        early_stop: bool\n",
        "        l2: bool\n",
        "        silent: bool. Controls whether or not to print the train and val error during training\n",
        "        \n",
        "        @return\n",
        "        a dictionary of arrays with train and val losses and accuracies\n",
        "        \"\"\"\n",
        "        ### convert data to tensor of correct shape and type here ###\n",
        "        inputs = torch.tensor(inputs, dtype=torch.float32)\n",
        "        outputs = torch.tensor(outputs, dtype=torch.long)\n",
        "        val_inputs = torch.tensor(val_inputs, dtype=torch.float32)\n",
        "        val_outputs = torch.tensor(val_outputs, dtype=torch.long)\n",
        "        \n",
        "        losses = []\n",
        "        accuracies = []\n",
        "        val_losses = []\n",
        "        val_accuracies = []\n",
        "        weights = self.model.state_dict()\n",
        "        lowest_val_loss = np.inf\n",
        "        \n",
        "        for n_epoch in tqdm(range(self.epoch), leave=False):\n",
        "            self.model.train()\n",
        "            batch_indices = list(range(inputs.shape[0]))\n",
        "            random.shuffle(batch_indices)\n",
        "            batch_indices = create_chunks(batch_indices, chunk_size=self.batch_size)\n",
        "            epoch_loss = 0\n",
        "            epoch_acc = 0\n",
        "            for batch in batch_indices:\n",
        "                batch_importance = len(batch) / len(outputs)\n",
        "                batch_input = inputs[batch]\n",
        "                batch_output = outputs[batch]\n",
        "                ### make prediction and compute loss with loss function of your choice on this batch ###\n",
        "                batch_predictions = self.model.forward(batch_input)\n",
        "                loss_func = nn.CrossEntropyLoss()\n",
        "                loss = loss_func(batch_predictions, batch_output)\n",
        "                if l2:\n",
        "                    ### Compute the loss with L2 regularization ###\n",
        "                    self.optimizer = torch.optim.Adam(model.parameters(), lr = self.learning_rate, weight_decay = 1e-5)\n",
        "                    loss = loss_func(batch_predictions, batch_output)\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                ### Compute epoch_loss and epoch_acc\n",
        "            epoch_loss, epoch_acc = self.evaluate(inputs, outputs)  \n",
        "            val_loss, val_acc = self.evaluate(val_inputs, val_outputs, print_acc=False)\n",
        "            if n_epoch % 10 ==0 and not silent: \n",
        "                print(\"Epoch %d/%d - Loss: %.3f - Acc: %.3f\" % (n_epoch + 1, self.epoch, epoch_loss, epoch_acc))\n",
        "                print(\"              Val_loss: %.3f - Val_acc: %.3f\" % (val_loss, val_acc))\n",
        "            losses.append(epoch_loss.detach().numpy())\n",
        "            accuracies.append(epoch_acc)\n",
        "            val_losses.append(val_loss.detach().numpy())\n",
        "            val_accuracies.append(val_acc)\n",
        "            if early_stop:\n",
        "                if val_loss < lowest_val_loss:\n",
        "                    lowest_val_loss = val_loss\n",
        "                    weights = self.model.state_dict()\n",
        "\n",
        "        if early_stop:\n",
        "            self.model.load_state_dict(weights)    \n",
        "\n",
        "        return {\"losses\": losses, \"accuracies\": accuracies, \"val_losses\": val_losses, \"val_accuracies\": val_accuracies}\n",
        "        \n",
        "    def evaluate(self, inputs, outputs, print_acc=False):\n",
        "        \"\"\" evaluate model on provided input and output\n",
        "        inputs: np.array, The shape of input_transform(input) should be (ndata,nfeatures)\n",
        "        outputs: np.array shape (ndata,)\n",
        "        print_acc: bool\n",
        "        \n",
        "        @return\n",
        "        losses: float\n",
        "        acc: float\n",
        "        \"\"\"\n",
        "\n",
        "        inputs = torch.tensor(inputs, dtype=torch.float32)\n",
        "        outputs = torch.tensor(outputs, dtype=torch.long)\n",
        "\n",
        "        loss_func = nn.CrossEntropyLoss()\n",
        "        \n",
        "        pred = self.model.forward(inputs)\n",
        "\n",
        "        losses = loss_func(pred, outputs)\n",
        "        #print(\"pred = \", pred)\n",
        "        #print(\"truth = \" ,outputs)\n",
        "        \n",
        "        sum = 0\n",
        "        for i in range(len(outputs)):\n",
        "            if outputs[i] == torch.argmax(pred[i]):\n",
        "                sum += 1\n",
        "        acc = sum / len(outputs)\n",
        "        if print_acc:\n",
        "            print(\"Accuracy: %.3f\" % acc)\n",
        "        return losses, acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa5a2a7f",
      "metadata": {
        "id": "aa5a2a7f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e11aea6",
      "metadata": {
        "id": "6e11aea6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}