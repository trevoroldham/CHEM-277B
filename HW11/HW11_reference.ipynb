{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD, Adam\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def data_gen(X,y, batchsize):\n",
    "    '''\n",
    "    Generator for data\n",
    "    '''\n",
    "    for i in range(len(X)//batchsize):\n",
    "        yield X[i*batchsize:(i+1)*batchsize],y[i*batchsize:(i+1)*batchsize]\n",
    "    i+=1\n",
    "    yield X[i*batchsize:],y[i*batchsize:]\n",
    "        \n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, model, optimizer_type, learning_rate, epoch, batch_size, input_transform=lambda x: x,):\n",
    "        \"\"\" The class for training the model\n",
    "        model: nn.Module\n",
    "            A pytorch model\n",
    "        optimizer_type: 'adam' or 'sgd'\n",
    "        learning_rate: float\n",
    "        epoch: int\n",
    "        batch_size: int\n",
    "        input_transform: func\n",
    "            transforming input. Can do reshape here\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        if optimizer_type == \"sgd\":\n",
    "            self.optimizer = SGD(model.parameters(), learning_rate,momentum=0.9)\n",
    "        elif optimizer_type == \"adam\":\n",
    "            self.optimizer = Adam(model.parameters(), learning_rate)\n",
    "        elif optimizer_type == 'adam_l2':\n",
    "            self.optimizer = Adam(model.parameters(), learning_rate, weight_decay=1e-5)\n",
    "            \n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.input_transform = input_transform\n",
    "\n",
    "\n",
    "    @timing\n",
    "    def train(self, inputs, outputs, val_inputs, val_outputs,draw_curve=False,early_stop=False,l2=False,silent=False):\n",
    "        \"\"\" train self.model with specified arguments\n",
    "        inputs: np.array, The shape of input_transform(input) should be (ndata,nfeatures)\n",
    "        outputs: np.array shape (ndata,)\n",
    "        val_nputs: np.array, The shape of input_transform(val_input) should be (ndata,nfeatures)\n",
    "        val_outputs: np.array shape (ndata,)\n",
    "        early_stop: bool\n",
    "        l2: bool\n",
    "        silent: bool. Controls whether or not to print the train and val error during training\n",
    "        \"\"\"\n",
    "        inputs = self.input_transform(torch.tensor(inputs, dtype=torch.float))\n",
    "        outputs = torch.tensor(outputs, dtype=torch.int64)\n",
    "        val_inputs = self.input_transform(torch.tensor(val_inputs, dtype=torch.float))\n",
    "        val_outputs = torch.tensor(val_outputs, dtype=torch.int64)\n",
    "\n",
    "        losses = []\n",
    "        val_losses = []\n",
    "        weights = self.model.state_dict()\n",
    "        lowest_val_loss = np.inf\n",
    "        \n",
    "        for n_epoch in tqdm(range(self.epoch), leave=False):\n",
    "            self.model.train()\n",
    "            #shuffle the data in each epoch\n",
    "            idx =torch.randperm(inputs.size()[0])\n",
    "            inputs=inputs[idx]\n",
    "            outputs=outputs[idx]\n",
    "            train_gen = data_gen(inputs,outputs,self.batch_size)\n",
    "            \n",
    "            epoch_loss = 0\n",
    "\n",
    "            for batch_input,batch_output in train_gen:\n",
    "                batch_importance = len(batch_output) / len(outputs)\n",
    "                batch_predictions, mu, logvar = self.model(batch_input)\n",
    "                loss = ...\n",
    "                if l2:\n",
    "                    l2_lambda = 1e-5\n",
    "                    l2_norm = sum(p.pow(2.0).sum() for p in self.model.parameters())\n",
    "                    loss = loss + l2_lambda * l2_norm\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_loss += loss.detach().cpu().item() /self.batch_size * batch_importance\n",
    "                \n",
    "            val_loss = self.evaluate(val_inputs, val_outputs, print_loss=False)\n",
    "            if n_epoch % 10 ==0 and not silent: \n",
    "                print(\"Epoch %d/%d - Loss: %.3f \" % (n_epoch + 1, self.epoch, epoch_loss))\n",
    "                print(\"              Val_loss: %.3f\" % (val_loss))\n",
    "            losses.append(epoch_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            if early_stop:\n",
    "                if val_loss < lowest_val_loss:\n",
    "                    lowest_val_loss = val_loss\n",
    "                    weights = self.model.state_dict()\n",
    "        if draw_curve:\n",
    "            plt.figure()\n",
    "            plt.plot(np.arange(self.epoch) + 1,losses,label='Training loss')\n",
    "            plt.plot(np.arange(self.epoch) + 1,val_losses,label='Validation loss')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "        \n",
    "        if early_stop:\n",
    "            self.model.load_state_dict(weights)    \n",
    "\n",
    "        return {\"losses\": losses,  \"val_losses\": val_losses}\n",
    "        \n",
    "    def evaluate(self, inputs, outputs, print_loss=True):\n",
    "        if torch.is_tensor(inputs):\n",
    "            inputs = self.input_transform(inputs)\n",
    "        else:\n",
    "            inputs = self.input_transform(torch.tensor(inputs, dtype=torch.float))\n",
    "            outputs = torch.tensor(outputs, dtype=torch.int64)\n",
    "        self.model.eval()\n",
    "        gen = data_gen(inputs,outputs,self.batch_size)\n",
    "        losses = 0\n",
    "\n",
    "        for batch_input,batch_output in gen:\n",
    "            batch_importance = len(batch_output) / len(outputs)\n",
    "            with torch.no_grad():\n",
    "                batch_predictions, mu, logvar = self.model(batch_input)\n",
    "                loss= ...\n",
    "            \n",
    "            losses += loss.detach().cpu().item()/self.batch_size * batch_importance\n",
    "\n",
    "        if print_loss:\n",
    "            print(\"Loss: %.3f\" % losses)\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from torchsummary import summary\n",
    "def train_model(model_func,Xs,ys,test_Xs,test_ys,epochs,draw_curve=True,early_stop=False,batchsize=128, optimizer='adam',lr=1e-3,l2=False,input_shape=(-1,1,32,32)):\n",
    "    train_Xs, val_Xs, train_ys, val_ys = train_test_split(Xs, ys, test_size=1/3, random_state=0)\n",
    "    model=model_func()\n",
    "    summary(model,input_shape[1:])\n",
    "\n",
    "    print(f\"{model_func.__name__} parameters:\", sum([len(item.flatten()) for item in model.parameters()]))\n",
    "            \n",
    "    trainer = Trainer(model, optimizer, lr, epochs, batchsize, lambda x: x.reshape(input_shape))\n",
    "    log=trainer.train(train_Xs, train_ys,val_Xs,val_ys,early_stop=early_stop,l2=l2)\n",
    "\n",
    "    if draw_curve:\n",
    "        plt.figure()\n",
    "        plt.plot(log[\"losses\"], label=\"losses\")\n",
    "        plt.plot(log[\"val_losses\"], label=\"validation_losses\")\n",
    "        plt.legend()\n",
    "        plt.title(f'loss')\n",
    "\n",
    "    # Report result for this fold\n",
    "    if early_stop:\n",
    "        report_idx= np.argmin(log[\"val_losses\"])      \n",
    "    else:\n",
    "        report_idx=-1\n",
    "    test_loss=trainer.evaluate(test_Xs,test_ys,print_loss=False)\n",
    "    print(\"Test loss:\",test_loss)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(vae,data_gen):\n",
    "    \"\"\"given a VAE model, plot original data and reconstructed data from VAE\"\"\"\n",
    "    inp = next(data_gen)[0]\n",
    "    print('Original Data:')\n",
    "    plot_digits(inp)\n",
    "    with torch.no_grad():\n",
    "        reconst,mu,log_var = vae(torch.tensor(inp,dtype=torch.float))\n",
    "\n",
    "    print('Reconstructed Data:')\n",
    "    plot_digits(reconst.detach().numpy()) \n",
    "    \n",
    "def plot_digits(data):\n",
    "    #plot 100 digit. data shape(100,32,32)\n",
    "    fig, ax = plt.subplots(10, 10, figsize=(12, 12),\n",
    "                           subplot_kw=dict(xticks=[], yticks=[]))\n",
    "    fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "    for i, axi in enumerate(ax.flat):\n",
    "        im = axi.imshow(data[i].reshape(32, 32), cmap=plt.get_cmap('gray'))\n",
    "        im.set_clim(0, 1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
