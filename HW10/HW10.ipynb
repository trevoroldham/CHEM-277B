{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CHEM277B Homework 10"
      ],
      "metadata": {
        "id": "yS31-vB2UdX4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (A)\n",
        "Process the smiles strings from ANI dataset by adding a starting character at the beginning\n",
        "and an ending character at the end. Look over the dataset and define the vocabulary, use one hot\n",
        "encoding to encode your smiles strings."
      ],
      "metadata": {
        "id": "FugPx5sAV5En"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import numpy as np\n",
        "from pyanitools import anidataloader\n",
        "\n",
        "data = anidataloader(\"ani_gdb_s06.h5\")\n",
        "data_iter = data.__iter__()\n",
        "#mols = next(data_iter)\n",
        "sm = []\n",
        "for mol in data:\n",
        "  # Extract the data\n",
        "  sm.append(mol['smiles'])\n",
        "sm_raw = sm\n"
      ],
      "metadata": {
        "id": "XdpPgx86UerA"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add SOS and EOS characters"
      ],
      "metadata": {
        "id": "tPa3HJiecLKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(sm))\n",
        "for idx in range(len(sm)):\n",
        "  sm[idx].insert(0, 'SOS')\n",
        "  sm[idx].append('EOS')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSwzhBpLXYAU",
        "outputId": "bd4ba4b0-33a7-4e5b-d6ca-af0ee4f6e3d2"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sm[0])\n",
        "print(type(sm[1][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZj9CbWa93g3",
        "outputId": "26298b3a-ece2-4409-b413-f292d24c735f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['SOS', '[', 'H', ']', 'N', '(', '[', 'H', ']', ')', 'C', '(', '[', 'H', ']', ')', '(', '[', 'H', ']', ')', 'C', '(', 'C', '(', '[', 'H', ']', ')', '(', '[', 'H', ']', ')', '[', 'H', ']', ')', '(', 'C', '(', '[', 'H', ']', ')', '(', '[', 'H', ']', ')', '[', 'H', ']', ')', 'C', '(', '[', 'H', ']', ')', '(', '[', 'H', ']', ')', '[', 'H', ']', 'EOS']\n",
            "<class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder()\n",
        "s = ['C', 'O', 'H', 'N', '[', ']', '(', ')', '#', '=', '0', '1', '2', 'c','h', 'o', 'n', \"EOS\", 'SOS']\n",
        "#s2 = ['C', 'O', 'H', 'N', '[', ']', '(', ')', '#', '=', '0', '1', '2', 'c','h', 'o', 'n']\n",
        "s_np = np.array(s).reshape(-1, 1)\n",
        "enc.fit(s_np)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "XvFxbRmYWOl7",
        "outputId": "f9f4641b-9fe4-4719-a194-b98224c6c0b4"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OneHotEncoder()"
            ],
            "text/html": [
              "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneHotEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "def batches_gen(smiles, batchsize, encoder):\n",
        "    '''Create a generator that returns batches of size (batch_size,seq_leng,nchars) from smiles, \n",
        "    where seq_leng is the length of the longest smiles string and nchar is the length of one-hot encoded characters (17)\n",
        "       \n",
        "       Arguments\n",
        "       ---------\n",
        "       smiles: python list(nsmiles,nchar) smiles array shape you want to make batches from\n",
        "       batchsize: Batch size, the number of sequences per batch\n",
        "       encoder: one hot encoder\n",
        "\n",
        "    '''\n",
        "    arr=[torch.tensor(np.array(encoder.transform(np.array(s).reshape(-1,1)).toarray()),dtype=torch.float) for s in smiles] \n",
        "        #size (nsmiles,seq_length(variable),nchars)\n",
        "        \n",
        "    # The features\n",
        "    X = [s[:-1,:] for s in arr]\n",
        "    # The targets, shifted by one\n",
        "    y = [s[1:,:] for s in arr]\n",
        "    # pad sequence so that all smiles are the same length\n",
        "    X = nn.utils.rnn.pad_sequence(X,batch_first=True)\n",
        "    y = nn.utils.rnn.pad_sequence(y,batch_first=True)\n",
        "\n",
        "    \n",
        "    for i in range(len(arr)//batchsize):\n",
        "        yield X[i*batchsize:(i+1)*batchsize],y[i*batchsize:(i+1)*batchsize]\n",
        "        \n",
        "    #drop last batch that is not the same size due to hidden state constraint\n"
      ],
      "metadata": {
        "id": "ifJM-8SFYfHx"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing batches_gen\n",
        "batches = batches_gen(smiles = sm_raw, batchsize = 1, encoder = enc)\n"
      ],
      "metadata": {
        "id": "pVJDmV3HY0vQ"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (B)\n",
        "Build an LSTM with one recurrent layer."
      ],
      "metadata": {
        "id": "ZkKjh9wWVt0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.n_layers = 1\n",
        "        self.n_hidden = 64\n",
        "        self.chars = ['C', 'O', 'H', 'N', '[', ']', '(', ')', '#', '=', '0', '1', '2', 'c','h', 'o', 'n', 'EOS', 'SOS']\n",
        "\n",
        "\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size = 19, \n",
        "            hidden_size = self.n_hidden,\n",
        "            num_layers = self.n_layers,\n",
        "            batch_first = True, #(batch, time_step, input_size)\n",
        "            \n",
        "\n",
        "        )\n",
        "        self.out = nn.Linear(64, 19)\n",
        "\n",
        "    def forward(self, x, h_state):\n",
        "        # x (batch, time_step, input_size)\n",
        "        # h_state (n_layers, batch, hidden_size)\n",
        "        # r_out (batch, time_step, hidden_size)\n",
        "        r_out, h_state = self.lstm(x, h_state)\n",
        "        outs = self.out(r_out)\n",
        "        #outs = nn.Softmax(dim=0)(outs)\n",
        "        return outs, h_state\n",
        "    \n",
        "    def init_state(self, batchsize):\n",
        "        return (torch.zeros(self.n_layers, batchsize, self.n_hidden), #hidden state\n",
        "                torch.zeros(self.n_layers, batchsize, self.n_hidden)) #cell state"
      ],
      "metadata": {
        "id": "iULJK_hkVtFw"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the LSTM Model"
      ],
      "metadata": {
        "id": "zNnpgOhqbE0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lstm = LSTM()\n",
        "h_state, c_state = lstm.init_state(128)\n",
        "print(lstm)\n",
        "\n",
        "optimizer = torch.optim.Adam(lstm.parameters(), lr = 0.01)\n",
        "loss_func = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtgx_aHuZxqN",
        "outputId": "85a59566-57ac-4fdb-d122-6129a6441519"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM(\n",
            "  (lstm): LSTM(19, 64, batch_first=True)\n",
            "  (out): Linear(in_features=64, out_features=19, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(50):\n",
        "    batches = batches_gen(smiles = sm, batchsize = 128, encoder = enc)\n",
        "    \n",
        "    for batch in batches:\n",
        "     \n",
        "      prediction, (h_state, c_state) = lstm(batch[0], (h_state, c_state))   # rnn output\n",
        "    # !! next step is important !!\n",
        "#    h_state = h_state.data        # repack the hidden state, break the connection from last iteration\n",
        "#     # you can also do\n",
        "      h_state = h_state.detach()\n",
        "      c_state = c_state.detach()\n",
        "      \n",
        "      loss = loss_func(prediction, batch[1])         # calculate loss\n",
        "      optimizer.zero_grad()                   # clear gradients for this training step\n",
        "      loss.backward()                         # backpropagation, compute gradients\n",
        "      optimizer.step()\n",
        "\n",
        "    if i % 5 == 0:\n",
        "      print(f\"Epoch {i} : Loss = {loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRjrymb5bH9x",
        "outputId": "e0a24aa8-727e-4f68-fdeb-451f4ab09bca"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 : Loss = 7.9578022956848145\n",
            "Epoch 5 : Loss = 4.4139275550842285\n",
            "Epoch 10 : Loss = 4.0715203285217285\n",
            "Epoch 15 : Loss = 3.9366157054901123\n",
            "Epoch 20 : Loss = 3.889498710632324\n",
            "Epoch 25 : Loss = 3.778305768966675\n",
            "Epoch 30 : Loss = 3.767561197280884\n",
            "Epoch 35 : Loss = 3.720902681350708\n",
            "Epoch 40 : Loss = 3.749427318572998\n",
            "Epoch 45 : Loss = 3.7441751956939697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a method to generate the next character\n",
        "def predict(net, inputs, h, top_k=None):\n",
        "        ''' Given a onehot encoded character, predict the next character.\n",
        "            Returns the predicted onehot encoded character and the hidden state.\n",
        "        Arguments:\n",
        "            net: the lstm model\n",
        "            inputs: input to the lstm model. shape (batch, time_step/length_of_smiles, input_size) with batchsize of 1\n",
        "            h: hidden state (h,c)\n",
        "            top_k: int. sample from top k possible characters\n",
        "            \n",
        "        '''\n",
        "        # detach hidden state from history\n",
        "        h = tuple([each.data for each in h])\n",
        "        # get the output of the model\n",
        "        out, h = net(inputs, h)\n",
        "        # get the character probabilities\n",
        "        p = out.data\n",
        "        p = nn.Softmax(dim=2)(p)\n",
        "\n",
        "        # get top characters\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(net.chars)) #index to choose from\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        # select the likely next character with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        # return the onehot encoded value of the predicted char and the hidden state\n",
        "        output = np.zeros(inputs.detach().numpy().shape)\n",
        "        output[:,:,char] = 1\n",
        "        output = torch.tensor(output,dtype=torch.float)\n",
        "        return output, h\n",
        "\n",
        "# Declaring a method to generate new text\n",
        "def sample(net, encoder, prime=['SOS'], top_k=None):\n",
        "    \"\"\"generate a smiles string starting from prime. I use 'SOS' (start of string) and 'EOS'(end of string). \n",
        "    You may need to change this based on your starting and ending character.\n",
        "\n",
        "    \"\"\"\n",
        "    net.eval() # eval mode\n",
        "    # get initial hidden state with batchsize 1\n",
        "    h = net.init_state(1)\n",
        "    # First off, run through the prime characters\n",
        "    chars=[]\n",
        "    for ch in prime:\n",
        "        ch = encoder.transform(np.array([ch]).reshape(-1, 1)).toarray() #(1,17)\n",
        "        ch = torch.tensor(ch,dtype=torch.float).reshape(1,1,19)\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "    chars.append(char)\n",
        "    end  = encoder.transform(np.array(['EOS']).reshape(-1, 1)).toarray()\n",
        "    end = torch.tensor(end,dtype=torch.float).reshape(1,1,19)\n",
        "\n",
        "    # Now pass in the previous character and get a new one\n",
        "    while not torch.all(end.eq(chars[-1])):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "    chars =[c.detach().numpy() for c in chars]\n",
        "    chars = np.array(chars).reshape(-1,19)\n",
        "    chars = encoder.inverse_transform(chars).reshape(-1)\n",
        "    return ''.join(chars[:-1])"
      ],
      "metadata": {
        "id": "hqFCMY7macQI"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test the LSTM with the sample method\n",
        "\n",
        "for i in range(20):\n",
        "  print(sample(lstm, enc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZZWgSvQadBT",
        "outputId": "8bf679fc-76b5-4cc2-cf4d-4d0cb4d67cd0"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=NN=N1\n",
            "N=C1\n",
            "O[1o\n",
            "O1\n",
            "OC2([H])c1\n",
            "N=C1\n",
            "OC1=NN([H])N([H])N1\n",
            "[H])N1o\n",
            "[H])o1\n",
            "OC#N\n",
            "0h0)C12N(02)n2\n",
            "[#])N1\n",
            "=C1\n",
            "OC1oc\n",
            "OC1\n",
            "=O\n",
            "C#N\n",
            "N2N(SOS[0h)[H]\n",
            "[H])N#\n",
            "N=NOC1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of the sample method is nonsensical strings that are not valid SMILES codes. There is an error somewhere in the code which is preventing the model from training further, although it does reduce the Cross Entropy Loss over 50 epochs, it does not find a good minimum and ceases to train past 50 epochs. One could consider two linear layers to increase the capacity of the LSTM model."
      ],
      "metadata": {
        "id": "nZO4O556b13w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iPWEa6ZK1zV9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}