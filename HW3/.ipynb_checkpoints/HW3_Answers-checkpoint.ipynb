{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "123715f0",
   "metadata": {},
   "source": [
    "# CHEM277B Homework 3\n",
    "### Trevor Oldham\n",
    "### February 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a726909",
   "metadata": {},
   "source": [
    "### Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd93ded",
   "metadata": {},
   "source": [
    "### (A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cd5e6a",
   "metadata": {},
   "source": [
    "List all good solutions with f(x) > 27 for each encoding:\n",
    "\n",
    "\n",
    "**Encoding A:**\n",
    "\n",
    "Solution 3: 1000\n",
    "\n",
    "Solution 4: 0010\n",
    "\n",
    "Solution 5: 0001\n",
    "\n",
    "Schema: (* 0 \\* \\*)\n",
    "\n",
    "Length: 0\n",
    "\n",
    "Order: 1\n",
    "\n",
    "**Encoding B:**\n",
    "\n",
    "Solution 3: 1101\n",
    "\n",
    "Solution 4: 1011\n",
    "\n",
    "Solution 5: 1111\n",
    "\n",
    "\n",
    "Schema: (1 \\* \\* 1)\n",
    "\n",
    "Length: 3\n",
    "\n",
    "Order: 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84378f0f",
   "metadata": {},
   "source": [
    "We want to choose a schema with short length and low order because those choices are less likely to be disrupted by genetic operations like splicing so we should choose Encoding A as the most appropriate encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0df42e",
   "metadata": {},
   "source": [
    "### (B)\n",
    "\n",
    "Drawing from the pool of candidates we can make pairs by pairing the most fit with the least fit, the second most fit with the second least fit, and so on.\n",
    "\n",
    "Solution 10: 0101 - fitness = -5\n",
    "\n",
    "Solution  1: 0011 - fitness = 22\n",
    "\n",
    "Solution 15: 1111 - fitness = -90\n",
    "\n",
    "Solution  6: 0000 - fitness = 27\n",
    "\n",
    "Solution  0: 1011 - fitness = 15\n",
    "\n",
    "Solution  9: 1100 - fitness = 6\n",
    "\n",
    "**Pairs**\n",
    "\n",
    "(6, 15) - (0000, 1111)\n",
    "\n",
    "(1, 10) - (0011, 0101)\n",
    "\n",
    "(0,  9) - (1011, 1100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14215955",
   "metadata": {},
   "source": [
    "### (C)\n",
    "\n",
    "Create a cross-over point after the first element of the encoding and swap the elements after that point.\n",
    "\n",
    "**Pairs**\n",
    "\n",
    "(6, 15) - (0000, 1111) - After cross-over (0111, 1000)\n",
    "\n",
    "(1, 10) - (0011, 0101) - After cross-over (0101, 0011)\n",
    "\n",
    "(0, 9) - (1011, 1100) - After cross-over (1100, 1011)\n",
    "\n",
    "The first pair results in two new solutions after the cross-over. The second pair and the third pair do not create new solutions. The two new solutions are 0111 (x=12) and 1000 (x=3). They have fitness f(12)=-33 and f(3)=30. The sum total of the fitness for the new generation is larger than the fitness before, and x=3 becomes the most fit member. The current population is now (12, 3, 10, 1, 9, 0) with a fitness of 35 total."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597058b1",
   "metadata": {},
   "source": [
    "### (D)\n",
    "\n",
    "Mutate the third element of the encodings for each string in the new population.\n",
    "\n",
    "Solution 12: 0111 - After mutation 0101 - f(1) = 15\n",
    "\n",
    "Solution  3: 1000 - After mutation 1010 - f(7) = 22\n",
    "\n",
    "Solution 10: 0011 - After mutation 0001 - f(5) = 30\n",
    "\n",
    "Solution  1: 0101 - After mutation 0111 - f(12) = -33\n",
    "\n",
    "Solution  9: 1100 - After mutation 1110 - f(14) = -69\n",
    "\n",
    "Solution  0: 1011 - After mutation 1001 - f(2) = 27\n",
    "\n",
    "There are new solutions (7, 5, 14, 2) and we no longer have (3, 10, 9, 6, 0). The total fitness of the new population is now -8 meaning the new population is less fit than the previous population."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b026355b",
   "metadata": {},
   "source": [
    "### (E)\n",
    "\n",
    "Eliminate the least fit member and replace with clone of the most fit member. Then create pairs and do two-point crossover on the two inner characters in the strings.\n",
    "\n",
    "Replace Solution 14 with Solution 5 (0001)\n",
    "\n",
    "**Pairs**\n",
    "\n",
    "(5, 1) - (0001, 0101) - After cross-over (0101, 0001) - (1, 5)\n",
    "\n",
    "(5, 12) - (0001, 0111) - After cross-over (0111, 0001) - (12, 5)\n",
    "\n",
    "(2, 7) - (1001, 1010) - After cross-over (1011, 1000) - (0, 3)\n",
    "\n",
    "**Fitness** \n",
    "\n",
    "f(0) = 15\n",
    "\n",
    "f(1) = 22\n",
    "\n",
    "f(3) = 30\n",
    "\n",
    "f(5) = 30\n",
    "\n",
    "f(5) = 30\n",
    "\n",
    "f(12) = -33\n",
    "\n",
    "Total = 94\n",
    "\n",
    "The new population contains (0, 1, 3, 5, 5, 12) and has a new fitness of 94 which is better than the previous generation and the generation before that. The current best solution is x=3 or x=5 with a fitness of 30.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f3c521",
   "metadata": {},
   "source": [
    "### (F)\n",
    "\n",
    "Eliminate the least fit member and replace with a clone of the most fit member. Then create pairs and do a cross-over of the first three elements in the string encoding.\n",
    "\n",
    "Replace Solution 12 with Solution 3 (1000)\n",
    "\n",
    "**Pairs**\n",
    "\n",
    "(5, 0) - (0001, 1011) - After crossover (1011, 1001) - (0, 2)\n",
    "\n",
    "(3, 1) - (1000, 0011) - After crossover (0010, 1001) - (4, 2)\n",
    "\n",
    "(3, 5) - (1000, 0001) - After crossover (0000, 1001) - (6, 2)\n",
    "\n",
    "**Fitness**\n",
    "\n",
    "f(0) = 15\n",
    "\n",
    "f(2) = 27\n",
    "\n",
    "f(2) = 27\n",
    "\n",
    "f(2) = 27\n",
    "\n",
    "f(4) = 31\n",
    "\n",
    "f(6) = 27\n",
    "\n",
    "Total = 154\n",
    "\n",
    "The new population is (0, 2, 2, 2, 4, 6) and the new population fitness is 154 which is the best generation yet. The most fit member is x=4 with a fitness of 31 which is the correct answer for maximizing the function given. The values seem to be converging to a range around 4 which contains the solutions closest to the true solution. This generation could be different depending on the pairings that were chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385c81bd",
   "metadata": {},
   "source": [
    "### (F)\n",
    "\n",
    "I think the encoding of the solution space was a good choice because the numbers seem to converge around the true solution at x=4. I imagine that the value of 4 would begin to appear more and more as subsquent iterations of cross-over and mutation are performed, because the Encoding A has a schema which defines the most fit members having a short length and a low order, which has been proven to guarantee an exponential increase in the most fit solutions. We could also replace the least fit member with the most fit member which would increase the occurrence of solution 4 as time goes on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f113d0",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a865df62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5fb4cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN():\n",
    "    def __init__(self, architecture, learning_rate, activation_function):\n",
    "        #initialize the model\n",
    "        self.arch = architecture\n",
    "        self.activation = activation_function\n",
    "        self.learning_rate = learning_rate\n",
    "        self.depth = len(self.arch)\n",
    "    \n",
    "    def init_weights(self):\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for l in range(self.depth - 1):\n",
    "            prev_layer_number = self.arch[l]\n",
    "            current_layer_number = self.arch[l + 1]\n",
    "            #tip: generate random matrix for weights rather than zeros for homework\n",
    "            self.weights.append(np.random.rand(current_layer_number, prev_layer_number))\n",
    "            self.biases.append(np.random.rand(current_layer_number))\n",
    "        #print(\"weights after init\")\n",
    "        #print(self.weights)\n",
    "    \n",
    "    def feed_forward(self, x):\n",
    "        self.z_s = []\n",
    "        self.a_s = [x]\n",
    "        for l in range(self.depth - 1):\n",
    "            z_l = np.dot(self.weights[l], (self.a_s[-1])) + self.biases[l]\n",
    "            a_l = self.activation(z_l)\n",
    "            self.z_s.append(z_l)\n",
    "            self.a_s.append(a_l)\n",
    "        #print(\"activations\")\n",
    "        #print(self.a_s)\n",
    "        return self.a_s[-1]\n",
    "    \n",
    "    def calc_error(self, y, activation_grad):\n",
    "        #todo\n",
    "        self.errors = [0] * (self.depth - 1)\n",
    "        self.errors[-1] = (self.a_s[-1] - y) *  activation_grad(self.z_s[-1])\n",
    "        \n",
    "        for l in range(self.depth - 2):\n",
    "            self.errors[l] = np.multiply(np.transpose(self.weights[l+1]).dot(self.errors[l+1]), activation_grad(self.z_s[l]))\n",
    "        #print('Errors after calc_error')\n",
    "        #print(self.errors)\n",
    "        \n",
    "    def calc_grad(self):\n",
    "        self.biases_grad = self.errors\n",
    "        self.weights_grad = []\n",
    "        \n",
    "        for l in range(self.depth - 1):\n",
    "            self.weights_grad.append(np.dot(self.a_s[l+1], self.errors[l]))\n",
    "        \n",
    "    def back_prop(self):\n",
    "        for l in range(self.depth - 1):\n",
    "            self.weights[l] = self.weights[l] - self.learning_rate * self.weights_grad[l]\n",
    "            self.biases[l] = self.biases[l] - self.learning_rate * self.biases_grad[l]\n",
    "    \n",
    "    def fit(self, x, y, activation_grad):\n",
    "        self.feed_forward(x)\n",
    "        self.calc_error(y, activation_grad)\n",
    "        self.calc_grad()\n",
    "        self.back_prop()\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.feed_forward(x)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d0f508d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized prediction: [0.64299999 0.79969983]\n",
      "Error in nodes [array([0.05582764, 0.57265422]), array([0.96370331, 0.64875612])]\n",
      "Prediction after fitting once [0.62330622 0.78936865]\n"
     ]
    }
   ],
   "source": [
    "def tan_h(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tan_h_grad(x):\n",
    "    return 1 - tan_h(x)**2\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "nn = NN([6, 2, 2], learning_rate = 0.05, activation_function = tan_h)\n",
    "nn.init_weights()\n",
    "x = [-1, 1, -1, -1, 1, -1]\n",
    "print(\"Initialized prediction:\", nn.predict(x))\n",
    "y = [-1, -1]\n",
    "nn.fit(x, y, tan_h_grad)\n",
    "print(\"Error in nodes\", nn.errors)\n",
    "print(\"Prediction after fitting once\", nn.predict(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69955e9",
   "metadata": {},
   "source": [
    "### (A)\n",
    "\n",
    "The weights are initialized in the function init_weights to be random values from 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79a2b7c",
   "metadata": {},
   "source": [
    "### (B)\n",
    "\n",
    "A new NN object is created and given the activation function, the inital x vector, and the learning rate. The weights are then initialized and then the funcion nn.predict(x) is called to compute the first prediction of the neural net before fitting. I chose the learning rate parameter to be 0.05 which resulted in an output which is closer to the reference output provided. From this initial prediction the neural net suggests that the predicted output is [0.64299999 0.79969983] which does not fall into the classifications of helix [1, -1], beta sheet [-1, 1], or coil [-1, -1] but can be rounded to [1, 1] which suggests that the secondary structure is hydophobic (1) and has propensity to form a helix (1). This is only half correct because we know that the observed structure for x is [-1, -1] which is a coil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e303403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized prediction: [0.64299999 0.79969983]\n"
     ]
    }
   ],
   "source": [
    "def tan_h(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tan_h_grad(x):\n",
    "    return 1 - tan_h(x)**2\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "nn = NN([6, 2, 2], learning_rate = 0.05, activation_function = tan_h)\n",
    "nn.init_weights()\n",
    "x = [-1, 1, -1, -1, 1, -1]\n",
    "print(\"Initialized prediction:\", nn.predict(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c23743",
   "metadata": {},
   "source": [
    "### (C)\n",
    "\n",
    "The error is calculated in the function nn.calc_error which creates an array of all the nodes which are not the input layer. A value for y is given as [-1, -1] and is passed to the nn.fit() function along with the initial vector x and the derivative of the activation function. Then the array of errors is reported to the user along with a new prediction from the neural net after one call to the nn.fit() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66d7948a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in nodes [array([0.05582764, 0.57265422]), array([0.96370331, 0.64875612])]\n",
      "Prediction after fitting once [0.62330622 0.78936865]\n"
     ]
    }
   ],
   "source": [
    "y = [-1, -1]\n",
    "nn.fit(x, y, tan_h_grad)\n",
    "print(\"Error in nodes\", nn.errors)\n",
    "print(\"Prediction after fitting once\", nn.predict(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30311c71",
   "metadata": {},
   "source": [
    "### (D)\n",
    "\n",
    "The weights is a list of arrays which contains all the weights from each layer except the final layer. The equation to calculate the new weights is given in the function nn.back_prop(), which uses the current weight values and the gradient of the cost function with respect to each weight which is an array nn.weights_grad. The equation to compute the weights_grad values is given in the reference reading as well as the equation to calculate the biases. The gradient of the cost function with respect to the biases is equal to the array nn.errors. The learning rate parameter is chosen to be 0.05 which results in a new prediction which is close to the suggested output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860b6ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
